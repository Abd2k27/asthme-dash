## Library ##
import requests
import time
import os
from datetime import datetime, timedelta
import pandas as pd
import io
import unidecode
import boto3

def upload_to_s3(local_file, bucket_name, s3_file_name):
    """
    Upload un fichier local vers un bucket S3.
    """
    if not os.path.exists(local_file):
        print(f"‚ùå Erreur : Le fichier {local_file} n'existe pas. Upload annul√©.")
        return
    
    s3 = boto3.client('s3')

    try:
        print(f"üì§ Upload en cours : {local_file} vers s3://{bucket_name}/{s3_file_name}...")
        s3.upload_file(local_file, bucket_name, s3_file_name)
        print(f"‚úÖ Upload r√©ussi : {s3_file_name} dans le bucket {bucket_name}.")
    except Exception as e:
        print(f"‚ùå Erreur lors de l'upload vers S3 : {e}")

## Setting ##

# Cl√© API
api_key = "Fh4Saf51BS4oqiq7TItLXyf5qetIuC9Y"

# Dictionnaire des polluants d'int√©r√™ts
polluants = {
    'code': ["01", "03", "08", "24", "39"],
    'name': ['SO2', 'NO2', 'O3', 'PM10', 'PM2.5']
}

# Dictionnaire des d√©partements fran√ßais
departements = {
    "01": "Ain",
    "02": "Aisne",
    "03": "Allier",
    "04": "Alpes-de-Haute-Provence",
    "05": "Hautes-Alpes",
    "06": "Alpes-Maritimes",
    "07": "Ard√®che",
    "08": "Ardennes",
    "09": "Ari√®ge",
    "10": "Aube",
    "11": "Aude",
    "12": "Aveyron",
    "13": "Bouches-du-Rh√¥ne",
    "14": "Calvados",
    "15": "Cantal",
    "16": "Charente",
    "17": "Charente-Maritime",
    "18": "Cher",
    "19": "Corr√®ze",
    "2A": "Corse-du-Sud",
    "2B": "Haute-Corse",
    "21": "C√¥te-d'Or",
    "22": "C√¥tes-d'Armor",
    "23": "Creuse",
    "24": "Dordogne",
    "25": "Doubs",
    "26": "Dr√¥me",
    "27": "Eure",
    "28": "Eure-et-Loir",
    "29": "Finist√®re",
    "30": "Gard",
    "31": "Haute-Garonne",
    "32": "Gers",
    "33": "Gironde",
    "34": "H√©rault",
    "35": "Ille-et-Vilaine",
    "36": "Indre",
    "37": "Indre-et-Loire",
    "38": "Is√®re",
    "39": "Jura",
    "40": "Landes",
    "41": "Loir-et-Cher",
    "42": "Loire",
    "43": "Haute-Loire",
    "44": "Loire-Atlantique",
    "45": "Loiret",
    "46": "Lot",
    "47": "Lot-et-Garonne",
    "48": "Loz√®re",
    "49": "Maine-et-Loire",
    "50": "Manche",
    "51": "Marne",
    "52": "Haute-Marne",
    "53": "Mayenne",
    "54": "Meurthe-et-Moselle",
    "55": "Meuse",
    "56": "Morbihan",
    "57": "Moselle",
    "58": "Ni√®vre",
    "59": "Nord",
    "60": "Oise",
    "61": "Orne",
    "62": "Pas-de-Calais",
    "63": "Puy-de-D√¥me",
    "64": "Pyr√©n√©es-Atlantiques",
    "65": "Hautes-Pyr√©n√©es",
    "66": "Pyr√©n√©es-Orientales",
    "67": "Bas-Rhin",
    "68": "Haut-Rhin",
    "69": "Rh√¥ne",
    "70": "Haute-Sa√¥ne",
    "71": "Sa√¥ne-et-Loire",
    "72": "Sarthe",
    "73": "Savoie",
    "74": "Haute-Savoie",
    "75": "Paris",
    "76": "Seine-Maritime",
    "77": "Seine-et-Marne",
    "78": "Yvelines",
    "79": "Deux-S√®vres",
    "80": "Somme",
    "81": "Tarn",
    "82": "Tarn-et-Garonne",
    "83": "Var",
    "84": "Vaucluse",
    "85": "Vend√©e",
    "86": "Vienne",
    "87": "Haute-Vienne",
    "88": "Vosges",
    "89": "Yonne",
    "90": "Territoire de Belfort",
    "91": "Essonne",
    "92": "Hauts-de-Seine",
    "93": "Seine-Saint-Denis",
    "94": "Val-de-Marne",
    "95": "Val-d'Oise",
    "97": "Outre-mer"
}

## Function ##

def clean_header(df):
    """
    Nettoie les en-t√™tes d'un dataFrame
    - Passe tout en minuscule
    - Remplace les espaces par des underscores
    - Supprime les accents
    - Retire les caract√®res sp√©ciaux non alphanum√©riques
    
    Entr√©e
    - df (dataFrame pandas) dataFrame √† nettoyer
    
    Sortie
    - df (dataFrame pandas) dateFrame nettoy√©
    """
    # Nettoyage des en-t√™tes
    df.columns = [
        unidecode.unidecode(col).lower().replace(" ", "_").replace("-", "_")
        for col in df.columns
    ]
    print("En-t√™tes nettoy√©s.")
    
    return df

def deduplicate_csv(csv):
    """
    Supprime les doublons dans un fichier csv en conservant :
    - En priorit√©, la ligne avec validite = 1
    - Sinon, la ligne avec la date_de_fin la plus r√©cente
    - Si deux lignes sont strictement identiques, une seule est conserv√©e.

    Entr√©e 
        csv (str) : chemin du fichier CSV √† traiter
    Sortie
        Le fichier csv est mis √† jour sans doublons.
    """
    if not os.path.exists(csv):
        raise FileNotFoundError(f"Le fichier {csv} n'existe pas.")

    # Charger le fichier CSV
    df = pd.read_csv(csv, sep=";", low_memory=False, encoding="utf-8")

    # Colonnes de date √† convertir
    date_columns = ['date_de_debut', 'date_de_fin']

    # Convertir les colonnes de date en datetime
    for col in date_columns:
        if col in df.columns:
            # Gestion des formats de date multiples
            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y/%m/%d %H:%M:%S')

    # V√©rifier si les colonnes n√©cessaires existent
    if 'validite' not in df.columns or 'date_de_fin' not in df.columns:
        raise ValueError("Les colonnes 'validite' ou 'date_de_fin' sont manquantes dans le fichier CSV.")

    # Tri prioritaire : validite = 1, puis date_de_fin la plus r√©cente
    df = df.sort_values(by=['validite', 'date_de_fin'], ascending=[False, False])

    # Suppression des doublons sur les colonnes cl√©s (date_de_debut, code_site, polluant)
    df = df.drop_duplicates(subset=['date_de_debut', 'code_site', 'polluant'], keep='first')

    # Suppression stricte des doublons restants (toutes colonnes identiques)
    df = df.drop_duplicates(keep='first')

    # Reconvertir les dates au format souhait√© avant sauvegarde
    for col in date_columns:
        if col in df.columns:
            df[col] = df[col].dt.strftime('%Y/%m/%d %H:%M:%S')  # Format uniforme YYYY/MM/DD HH:MM:SS

    # Sauvegarde du fichier mis √† jour
    df.to_csv(csv, sep=";", index=False, encoding="utf-8")
    print(f"Fichier {csv} d√©dupliqu√©.")

def reorder_csv(csv):
    """
    Trie un fichier csv :
    - Par date_de_debut (ordre chronologique)
    - Puis par organisme, code_zas, zas, code_site, nom_site et polluant (ordre alphab√©tique)

    Entr√©e 
        csv (str) chemin du fichier CSV √† traiter
        
    Sortie 
        Le fichier csv est mis √† jour et tri√©
    """
    if not os.path.exists(csv):
        raise FileNotFoundError(f"Le fichier {csv} n'existe pas.")

    # Charger le fichier CSV
    df = pd.read_csv(csv, sep=";", low_memory=False, encoding="utf-8")

    # Colonnes de date √† convertir
    date_columns = ['date_de_debut', 'date_de_fin']

    # Convertir les colonnes de date en datetime
    for col in date_columns:
        if col in df.columns:
            # Gestion des formats de date multiples
            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y/%m/%d %H:%M:%S')

    # V√©rification de la pr√©sence des colonnes n√©cessaires
    # V√©rification de la pr√©sence des colonnes n√©cessaires
    required_columns = ["date_de_debut", "organisme", "code_zas", "zas", "code_site", "nom_site", "polluant"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        raise ValueError(f"Colonnes manquantes : {', '.join(missing_columns)}")

    # Tri du dataframe
    df = df.sort_values(by=required_columns, ascending=True)

    # Reconvertir les dates au format souhait√© avant sauvegarde
    for col in date_columns:
        if col in df.columns:
            df[col] = df[col].dt.strftime('%Y/%m/%d %H:%M:%S')  # Format uniforme YYYY/MM/DD HH:MM:SS

    # Sauvegarde du fichier mis √† jour
    df.to_csv(csv, sep=";", index=False, encoding="utf-8")
    print(f"Fichier {csv} tri√©.")

def fetch_station(date = datetime.today()):
    """
    R√©cup√®re les informations √† date des stations de mesure de pollution 
    - Cr√©e un csv pr√©cisant la date de mise √† jour de la base de donn√©e
    - Fournit les coordonn√©es GPS, commune et code de commune pour localiser les stations
    - Fournit diverses informations sur les conditions de recueil des concentrations de polluants

    Entr√©e
        date (datetype, optionnel): date de mise √† jour sur l'API de la liste des stations de recueil de donn√©es. Defaults to datetime.today()
    
    Sortie
        Le fichier csv est mis √† jour √† la date 
    """
    gen_url = "https://www.geodair.fr/api-ext/station/export"  # URL de l'API pour les stations
    date_str = date.strftime("%Y-%m-%d")  # date du jour au format YYYY-MM-DD
    csv = "geodair_station.csv"  # nom du fichier csv √† mettre √† jour par remplacement

    # En-t√™tes de la requ√™te
    headers = {
        "accept": "text/csv",  # Indiquer qu'on attend un fichier CSV
        "apikey": api_key
    }

    # Envoyer la requ√™te
    response = requests.get(f"{gen_url}?date={date_str}", headers=headers)

    # V√©rifier l'√©tat de la requ√™te
    if response.status_code == 200:
        print(f"Fichier t√©l√©charg√© avec succ√®s : {csv}")
        df = pd.read_csv(io.StringIO(response.text), sep=";", encoding="utf-8", low_memory=False)
        df = clean_header(df)
        df.to_csv(csv, sep=";", header=True, index=False, encoding="utf-8", lineterminator="\n")
    else:
        print(f"Erreur lors de la g√©n√©ration du fichier : {response.status_code} - {response.reason} - {response.text}")

    return

def merge_polluant_station(csv_polluant):
    """
    Ajoute les informations g√©ographiques aux fichiers de polluant horaire
    
    Entr√©e
        csv_polluant (str) chemin du fichier de polluant √† compl√©ter avec les donn√©es g√©ographiques de la station
    
    Sortie
        Le fichier csv des polluants est mis √† jour avec les informations g√©ographiques des stations de pr√©l√©vement
    """
    # Charger les fichiers csv
    df_polluant = pd.read_csv(csv_polluant, sep=";", low_memory=False, encoding="utf-8")
    csv_station = "geodair_station.csv"
    df_station = pd.read_csv(csv_station, sep=";", low_memory=False, encoding="utf-8")

    # D√©finir les colonnes √† ajouter
    cols_to_add = ["code_commune", "commune", "longitude", "latitude", "code_departement", "departement"]

    # Ajouter une colonne 'code_departement' depuis les 2 premiers chiffres de 'code_commune'
    df_station['code_departement'] = df_station['code_commune'].dropna().astype(str).str[:2]
    
    # Ajouter une colonne 'departement' √† partir du dictionnaire des d√©partements
    df_station['departement'] = df_station['code_departement'].map(departements).fillna("Inconnu")

    # Supprimer les colonnes existantes dans df_polluant pour √©viter les doublons
    df_polluant.drop(columns=[col for col in cols_to_add if col in df_polluant.columns], inplace=True)

    # Faire la jointure
    df_merged = df_polluant.merge(
        df_station[['code'] + cols_to_add], 
        left_on='code_site', 
        right_on='code', 
        how='left'
    ).drop(columns=['code'])  # Supprime la colonne 'code' dupliqu√©e

    # Remplacer les valeurs manquantes par celles du dataframe joint
    for col in cols_to_add:
        if col in df_polluant.columns:
            df_merged[col] = df_polluant[col].fillna(df_merged[col])

    # Sauvegarder le fichier
    df_merged.to_csv(csv_polluant, sep=";", header=True, index=False, encoding='utf-8', lineterminator="\n")

    print(f"Fichier {csv_polluant} compl√©t√© √† partir de {csv_station}.")

    return

def fetch_hour_today(date=datetime.today()):
    """
    R√©cup√®re les valeurs horaires des polluants via l'API Geodair
    - Met √† jour les donn√©es des stations de pr√©l√©vements
    
    Entr√©e
        date (datetime, optionnel) date des donn√©es √† r√©cup√©rer (par d√©faut, aujourd‚Äôhui)
        
    Sortie
        Le fichier csv horaire des polluants est mise √† jour par r√©√©criture 
    """
    gen_url = "https://www.geodair.fr/api-ext/MoyH/export"  # URL de l'API pour les moyennes horaires par date et par polluant
    dwl_url = "https://www.geodair.fr/api-ext/download"  # URL de l'API pour le t√©l√©chargement des donn√©es
    date_str = date.strftime("%Y-%m-%d")  # mise au format n√©cessaire pour adresser la requ√™te d'id
    csv = "geodair_hour.csv"  # nom du fichier csv √† mettre √† jour
    file_exists = False  # variable de diff√©renciation de la boucle pour initialiser les en-t√™tes
    
    # Supprime le fichier pr√©c√©dent s'il existe
    if os.path.exists(csv):
        os.remove(csv)
        print(f"Fichier {csv} supprim√©.")

    # Mise √† jour des donn√©es li√©es aux stations
    fetch_station()
    
    print(f"Traitement des donn√©es horaires : {date_str}")
    for i in range(len(polluants['code'])):
        code = polluants['code'][i]
        name = polluants['name'][i]

        headers = {"apikey": api_key}
        params = {"date": date_str, "polluant": code}

        print(f"Demande de g√©n√©ration du fichier : {name}")
        response = requests.get(gen_url, headers=headers, params=params)  # param√©trage de l'acc√®s √† l'API

        if response.status_code == 200:
            file_id = response.text.strip()
            print(f"G√©n√©ration du lien d'acc√®s aux donn√©es : {name}")

            while True:
                download_response = requests.get(dwl_url, headers=headers, params={"id": file_id})
                if download_response.status_code == 200:
                    df = pd.read_csv(io.StringIO(download_response.text), sep=";", encoding="utf-8", low_memory=False)
                    df = clean_header(df)
                    print(f"R√©cup√©ration des donn√©es : {name}")
                    # Ecrire dans le csv avec l'en-t√™te seulement pour la premi√®re it√©ration
                    df.to_csv(csv, sep=';', mode='a', header=not file_exists, index=False, encoding='utf-8', lineterminator="\n")
                    # Mise √† jour du flag apr√®s la premi√®re √©criture
                    file_exists = True
                    print(f"Fichier {csv} compl√©t√© pour {name}.")                        
                    break
                elif download_response.status_code == 202:
                    print("Le fichier n'est pas encore pr√™t. Nouvelle tentative dans 5 secondes...")
                    time.sleep(5)
                else:
                    print(f"Erreur lors de la r√©cup√©ration du fichier {name} : {download_response.status_code} - {download_response.reason} - {download_response.text}")
                    break
        else:
            print(f"Erreur lors de la g√©n√©ration du lien : {name} : {response.status_code} - {response.reason} - {download_response.text}")
    
    # Fusion avec donn√©es de localisation
    merge_polluant_station(csv)
    # V√©rification des doublons
    deduplicate_csv(csv)
    # Ordonnement des donn√©es
    reorder_csv(csv)  
    # Calcul de l'IQA par site de pr√©l√©vement
    update_iqa(csv) 
    
    return

def fetch_max_yesterday(date = datetime.today()-timedelta(days=1)):
    """R√©cup√®re le pic horaire de chaque polluant d'int√©r√™t via l'API geodair
    - Ajoute les donn√©es √† l'historique journalier

    Entr√©e
        date (datetime, optionnel) date de recueil des donn√©es √† collecter (par d√©faut la veille) = datetime(year, month, day)
        
    Sortie
        Le fichier csv journalier des polluants est mis √† jour par ajout de ligne de donn√©e
    """   
    gen_url = "https://www.geodair.fr/api-ext/MaxJH/export"  # URL de l'API pour valeurs moyennes horaire des polluants
    dwl_url = "https://www.geodair.fr/api-ext/download"  # URL de l'API pour le t√©l√©chargement des donn√©es
    date_str = date.strftime("%Y-%m-%d")  # date du jour au format YYYY-MM-DD
    csv = "geodair_max_daily.csv"  # nom du fichier de travail (pics d'enregistrement de polluant journalier √† mettre √† jour)

    # Mise √† jour des donn√©es li√©es aux stations
    fetch_station()

    print(f"Traitement des donn√©es journali√®re : {date_str}")
    for i in range(len(polluants['code'])):
        code = polluants['code'][i]
        name = polluants['name'][i]

        headers = {"apikey": api_key}
        params = {"date": date_str, "polluant": code}

        print(f"Demande de g√©n√©ration du fichier : {name}")
        response = requests.get(gen_url, headers=headers, params=params)  # param√©trage de l'acc√®s √† l'API

        if response.status_code == 200:
            file_id = response.text.strip()
            print(f"G√©n√©ration du lien d'acc√®s aux donn√©es : {name}")

            while True:
                download_response = requests.get(dwl_url, headers=headers, params={"id": file_id})

                if download_response.status_code == 200:
                    df = pd.read_csv(io.StringIO(download_response.text), sep=";", encoding="utf-8", low_memory=False)
                    df = clean_header(df)                    
                    print(f"R√©cup√©ration des donn√©es : {name}")   
                    df.to_csv(csv, sep=";", mode='a', header=not os.path.exists(csv), index=False, encoding='utf-8')
                    print(f"Fichier {csv} renseign√© pour {name}.")
                    break
                elif download_response.status_code == 202:
                    print("Le fichier n'est pas encore pr√™t. Nouvelle tentative dans 5 secondes...")
                    time.sleep(5)
                else:
                    print(f"Erreur lors de la r√©cup√©ration du fichier {name} : {download_response.status_code} - {download_response.reason} - {download_response.text}")
                    break
        else:
            print(f"Erreur lors de la r√©cup√©ration du fichier {name} : {response.status_code} - {response.reason} - {response.text}")

    # Fusion avec donn√©es de localisation
    merge_polluant_station(csv)
    # V√©rification des doublons
    deduplicate_csv(csv)
    # Ordonnement des donn√©es
    reorder_csv(csv)  
    # Calcul de l'IQA par site de pr√©l√©vement
    update_iqa(csv)
    # Met √† jour les donn√©es hebdomadaires
    aggregate_weekly()
    
    return

def aggregate_weekly():
    """
    Aggr√®ge les valeurs journali√®res de la semaine pass√©e en maximum hebdomadaire pour chaque site et chaque polluant
    - Ajoute les donn√©es √† l'historique hebdomadaire au format csv
    
    Sortie
        Le fichier csv hedomadaire des polluants est mis √† jour √† partir du fichier csv journalier
    """
    csv_daily = "geodair_max_daily.csv"  # nom du fichier de travail (pics d'enregistrement de polluant journalier √† aggr√©ger)
    csv_weekly = "geodair_max_weekly.csv"  # nom du fichier de travail (pics d'enregistrement de polluant hebdomadaire √† mettre √† jour)
    df = pd.read_csv(csv_daily, sep=";", parse_dates=['date_de_debut'], low_memory=False, encoding="utf-8")  # chargement csv > dataFrame
    
    # Mettre au format la colonne 'date_de_debut' en datetime
    df['date_de_debut'] = pd.to_datetime(df['date_de_debut'], format="%Y/%m/%d %H:%M:%S")  

    # Ajouter la colonne 'semaine' au format ann√©e-semaine
    df['semaine'] = df['date_de_debut'].dt.strftime('%Y-S%U')

    # D√©finir les colonnes √† conserver lors du regroupement
    groupby_cols = [
        "semaine", "organisme", "code_zas", "zas", "code_site", "nom_site",
        "type_d'implantation", "polluant", "type_d'influence", "discriminant",
        "reglementaire", "type_d'evaluation", "procedure_de_mesure", "type_de_valeur",
        "unite_de_mesure", "taux_de_saisie", "couverture_temporelle", "couverture_de_donnees",
        "code_qualite", "validite", "code_commune", "commune", "longitude", "latitude", "code_departement", "departement"
    ]

    # Agr√©ger les donn√©es par semaine et par site avec la valeur maximale
    df_weekly = df.groupby(groupby_cols, as_index=False).agg(
        max_week=('valeur', 'max')
    )

    # V√©rifier si l'historique hebdomadaire existe d√©j√†
    if os.path.exists(csv_weekly):
        df_hist = pd.read_csv(csv_weekly, sep=";", low_memory=False, encoding="utf-8")
        # Concat√©ner et √©viter les doublons
        df_final = pd.concat([df_hist, df_weekly], ignore_index=True)
        df_final = df_final.drop_duplicates(subset=["semaine", "code_site", "nom_site", "polluant"], keep='last')
    else:
        df_final = df_weekly

    # R√©√©criture de l'historique
    df_final.to_csv(csv_weekly, sep=";", mode='w', header=True, index=False, encoding='utf-8', lineterminator="\n")
    print(f"Fichier {csv_weekly} compl√©t√© √† partir de {csv_daily}.")

    return

def update_iqa(csv_polluant="geodair_max_daily.csv"):
    """
    Calcule l'IQA pour chaque couple date/site et g√©n√®re un nouveau fichier CSV avec l'IQA ajout√©.

    Entr√©e:
        csv_polluant (str): Chemin du fichier CSV √† traiter (`geodair_hour.csv` ou `geodair_max_daily.csv` par d√©faut).

    Sortie:
        Un nouveau fichier CSV est cr√©√© avec une ligne par date/station pour l'IQA.
    """
    # V√©rifier si le fichier existe
    if not os.path.exists(csv_polluant):
        raise FileNotFoundError(f"Le fichier {csv_polluant} n'existe pas.")

    # D√©terminer le nom du fichier de sortie en fonction du fichier d'entr√©e
    if "daily" in csv_polluant:
        csv_iqa = csv_polluant.replace("max", "iqa")
    elif "weekly" in csv_polluant:
        csv_iqa = csv_polluant.replace("max", "iqa")
    else:
        csv_iqa = csv_polluant.replace(".csv", "_iqa.csv")

    # Charger le fichier CSV
    df = pd.read_csv(csv_polluant, sep=";", low_memory=False, encoding="utf-8")

    # Colonnes de date
    date_columns = ['date_de_debut', 'date_de_fin']

    # Convertir les colonnes de date en datetime pour traitement
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y/%m/%d %H:%M:%S')

    # Convertir 'valeur' en float pour √©viter les erreurs de calcul
    df['valeur'] = pd.to_numeric(df['valeur'], errors='coerce')
    df = df.dropna(subset=['valeur'])  # Supprimer les lignes o√π 'valeur' est NaN

    # D√©finition des seuils IQA pour chaque polluant
    IQA_THRESHOLDS = {
        "PM10": [0, 20, 40, 50, 100, 150, 200],
        "PM2.5": [0, 10, 20, 25, 50, 75, 100],
        "NO2": [0, 40, 90, 120, 230, 340, 400],
        "O3": [0, 50, 100, 130, 240, 380, 500],
        "SO2": [0, 50, 100, 150, 200, 300, 400],
    }

    # Fonction pour calculer l'IQA
    def get_iqa(value, thresholds):
        for i in range(len(thresholds) - 1):
            if value <= thresholds[i + 1]:
                return i * 50
        return 300

    # Fonction pour obtenir la gravit√© de l'IQA
    def get_gravite(iqa):
        if iqa <= 50:
            return "Bon"
        elif iqa <= 100:
            return "Mod√©r√©"
        elif iqa <= 150:
            return "Mauvais"
        elif iqa <= 200:
            return "Tr√®s mauvais"
        elif iqa <= 300:
            return "Dangereux"
        return "Tr√®s dangereux"

    # Calculer l'IQA pour chaque groupe de donn√©es
    df_iqa = df.groupby(
        ['date_de_fin', 'code_departement']
    ).apply(lambda group: pd.Series({
        **{col: group[col].iloc[0] for col in group.columns if col not in ['polluant', 'valeur', 'unite_de_mesure']},  # Conserver toutes les colonnes sauf polluant, valeur, unit√©
        'indice_qualite_air': 'IQA',
        'valeur': max(get_iqa(row['valeur'], IQA_THRESHOLDS.get(row['polluant'].upper(), [0, 50, 100, 150, 200, 300, 400]))
                      for _, row in group.iterrows()),
        'risque': get_gravite(max(get_iqa(row['valeur'], IQA_THRESHOLDS.get(row['polluant'].upper(), [0, 50, 100, 150, 200, 300, 400])) for _, row in group.iterrows())) 
    }), include_groups=False).reset_index(drop=True) 

    # R√©int√©gration du format de date original
    for col in date_columns:
        if col in df_iqa.columns:
            df_iqa[col] = df_iqa[col].dt.strftime('%Y/%m/%d %H:%M:%S')

    # Sauvegarder le fichier mis √† jour dans un nouveau fichier CSV
    df_iqa.to_csv(csv_iqa, sep=";", mode='w', header=True, index=False, encoding='utf-8', lineterminator="\n")
    print(f"Fichier {csv_iqa} compl√©t√© √† partir de {csv_polluant}.")
    
    return

def load_data_from_s3(BUCKET_NAME="bucket-asthme-scraping", FILE_KEY=None, file_type="csv", parse_dates=None):
    """Fonction pour charger les donn√©es depuis le serveur S3"""
    s3 = boto3.client('s3')
    
    if file_type == "csv":
        obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)
        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')), sep=";", parse_dates=parse_dates)
    elif file_type == "excel":
        local_file = "/tmp/geodes_complet.xlsx"
        s3.download_file(BUCKET_NAME, FILE_KEY, local_file)
        df = pd.read_excel(local_file, engine="openpyxl")
    else:
        raise ValueError("Type de fichier non support√©. Utilisez 'csv' ou 'excel'.")
    
    return df

def get_max_daily_from_s3(csv = "geodair_max_daily.csv"):
    """Chargement des donn√©es"""
    df = load_data_from_s3(FILE_KEY=csv, file_type="csv", parse_dates=["date_de_debut", "date_de_fin"])
    df.to_csv(csv, sep=";", mode='w', header=not os.path.exists(csv), index=False, encoding='utf-8')
    
## Application ##

fetch_max_yesterday() #1/j √† 12h

# Upload des fichiers g√©n√©r√©s vers AWS S3
files_to_upload = [
    "geodair_station.csv",
    "geodair_max_daily.csv",
    "geodair_max_weekly.csv",
    "geodair_iqa_daily.csv"
]

bucket_name = "bucket-asthme-scraping"

for local_file in files_to_upload:
    upload_to_s3(local_file, bucket_name, local_file)